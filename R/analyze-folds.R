#' This function will be called from tlmixture() on each CV-TMLE split.
#'
#' @param splits tbd
#' @param outcome name of the outcome variable
#' @param exposures tbd
#' @param family tbd
#' @param quantiles_mixtures tbd
#' @param quantiles_exposures tbd
#' @param estimator_outcome tbd
#' @param estimator_propensity tbd
#' @param mixture_fn tbd
#' @param cluster_exposures tbd
#' @param folds_sl tbd
#' @param verbose tbd
#'
#' @importFrom SuperLearner SuperLearner SuperLearner.CV.control
#' @importFrom magrittr %>%
#' @importFrom dplyr ntile
#' @importFrom stats quantile predict
analyze_folds =
  function(splits, outcome, exposures,
           family,
           quantiles_mixtures,
           quantiles_exposures,
           estimator_outcome,
           estimator_propensity,
           mixture_fn,
           cluster_exposures,
           folds_sl = 2L,
           verbose = FALSE) {

    if (verbose) {
      cat("Analyzing a training/test split.\n")
    }

    # Training data split.
    data_train = rsample::analysis(splits)

    # Test / validation split.
    data_test = rsample::assessment(splits)

    # Define groups of exposures (skip this step by default)
    if (cluster_exposures) {
      stop("Automatic exposure clustering is not yet implemented.")
      # TBD.
      exposure_groups = NA
    } else if (is.list(exposures)) {
      # Exposures is already a list with each element being a group.
      exposure_groups = exposures
    } else {

      # Single group with all of the exposures.
      exposure_groups = list(exposures)
      # Add a name that will go into the group table.
      names(exposure_groups) = "all"
    }

    # A single vector with all exposure names.
    # (Will be the same as "exposures" if there is a single group, but if there
    # are multiple groups this vector version will be helpful.)
    all_exposures = unlist(exposure_groups)

    if (verbose) {
      cat("Exposure groups to analyze:", length(exposure_groups), "\n")
    }

    weights = list()

    # Quantile cutpoints for the mixture.
    cutpoints = list()
    test_results = list()
    # Status of the estimation of each exposure group
    group_status = list()

    # Loop over exposure groups.
    for (group_i in seq_along(exposure_groups)) {
      group_name = names(exposure_groups)[group_i]

      exposure_names = exposure_groups[[group_i]]

      if (verbose) {
        cat("Analyzing exposure group:", group_name, "\n")
      }

      # Convert each exposure in this group to quantiles.
      # TODO: extract the quantile probabilities so that they can be applied
      # to the test set.
      # NOTE: we are currently not using these quantiles.
      exp_quantiles = data_train[, exposure_names] %>%
        purrr::map_df(dplyr::ntile, quantiles_exposures)

      # Create exposure weights, hopefully resulting in a convex combination or
      # something similar.
      #result = create_exposure_weights(data_train,
      result = mixture_fn(data_train,
                          outcome,
                          exposures = exposure_names,
                          # Pass in all exposure groups in case they want to use for adjustment.
                          exposure_groups = exposure_groups,
                          quantiles = exp_quantiles,
                          # May or may not be used.
                          family = family,
                          verbose = verbose)

      # Extract any weights that were generated by the mixture estimator.
      weights[[group_name]] = result$weights

      # Create mixture on the training data.
      # For binomial GLM this will be on the linear (logit) scale by default.
      mixture_train = try({ predict(result, data_train) })

      if (class(mixture_train) == "try-error") {
        error_msg = "Failed predicting mixture on training data"
        # Save this result for auditing purposes.
        group_status[[group_i]] = list(
          status = "failed",
          message = error_msg
        )
        cat("Error:", error_msg, "\n")
        print(mixture_train)
        # Continue to the next exposure group.
        next
      }

      if (sd(mixture_train) == 0) {
        error_msg = "Mixture applied to training data has no variation."
        # Save this result for auditing purposes.
        group_status[[group_name]] = list(
          status = "failed",
          message = error_msg
        )
        cat("Error:", error_msg, "\n")
        # Continue to the next exposure group.
        next
      }

      # Check for missing values.
      if (sum(is.na(mixture_train)) > 0) {
        cat("Error: found missing values in the mixture prediction on training data.")
        browser()
      }

      # Create mixture on the test data.
      mixture_test = predict(result, data_test)

      if (length(dim(mixture_test)) > 1) {
        cat("Error: mixture prediction should be a vector, not a matrix.")
        browser()
      }

      if (sum(is.na(mixture_test)) > 0) {
        cat("Error: found missing values in the mixture prediction on test data.")
        browser()
      }

      # Calculate quantiles to discretize the continuous mixture.
      quantiles = quantile(mixture_train,
                           probs = seq(0, 1, by = 1 / quantiles_mixtures))

      # Check if we have overlap of cutpoints.
      # TODO: catch this error and continue on, at least with the other exposure groups.
      if (length(unique(quantiles)) < length(quantiles)) {
        print(hist(mixture_train))
        cat("Total quantiles:", length(quantiles), "Unique quantiles:", length(unique(quantiles)), "\n")
        cat("Quantiles:", quantiles, "\n")
        cat("Exposure group:", names(exposure_groups)[group_i], "\n")

        error_msg = "Mixture quantiles are not unique on the training data."

        # Save this result for auditing purposes.
        group_status[[group_name]] = list(
          status = "failed",
          message = error_msg
        )
        cat("Error:", error_msg, "\n")

        # Provide extended info.
        cat("Meaning that there is little variation in the mixture, likely due",
            "to it not being related to the outcome. To resolve, you could remove",
            "these exposures, change the mixture estimation function, or reduce",
            "the number of CV-TMLE folds.\n")

        # Continue to the next exposure group.
        next
      }


      cutpoints[[group_name]] = quantiles

      # Discretize into quantiles (quantiles_mixtures)
      # If we don't set include.lowest = TRUE then the lowest obs will have an NA.
      # TODO: can generate error - quantiles are not unique.
      # We use .bincode() to avoid the "quantiles are not unique" possible error.
      # See https://stackoverflow.com/questions/16184947/cut-error-breaks-are-not-unique
      mixture_bins = cut(mixture_train, breaks = quantiles, include.lowest = TRUE)
      #mixture_bins = .bincode(mixture_train, breaks = quantiles, include.lowest = TRUE)
      mixture_bins_test = cut(mixture_test, breaks = quantiles, include.lowest = TRUE)
      #mixture_bins_test = .bincode(mixture_test, breaks = quantiles, include.lowest = TRUE)

      # Check if we have fewer bins in the test data than in the training data.
      # If so, we may have too many CV-TMLE folds for this dataset size.
      if (length(unique(mixture_bins)) > length(unique(mixture_bins_test))) {
        cat("Training mixture bins:", length(unique(mixture_bins)),
            "Test mixture bins:", length(unique(mixture_bins_test)), "\n")
        stop(paste0("Error: test data does not cover all training data bins. ",
                    "Try reducing the number of CV-TMLE folds, or change the mixture estimation function."))
      }

      # If there is a mixture value in the test data that is outside of the training data bounds
      # we can round it to the nearest bin.
      # TODO: we may want to bound the test set mixture rather than assign bins directly.
      if (sum(mixture_test > max(mixture_train)) > 0) {
        test_mixture_too_high = which(mixture_test > max(mixture_train))
        # Slightly complex because cut() returns a factor.
        mixture_bins_test[test_mixture_too_high] = levels(mixture_bins)[max(as.numeric(mixture_bins), na.rm = TRUE)]
      }

      if (sum(mixture_test < min(mixture_train)) > 0) {
        test_mixture_too_low = which(mixture_test < min(mixture_train))
        # Slightly complex because cut() returns a factor.
        mixture_bins_test[test_mixture_too_low] = levels(mixture_bins)[min(as.numeric(mixture_bins), na.rm = TRUE)]
      }

      # Warn if there are missing values.
      if (sum(is.na(mixture_bins)) > 0) {
        cat("Warning: found NAs in the training mixture bins.\n")
        browser()
      }
      if (sum(is.na(mixture_bins_test)) > 0) {
        cat("Warning: found NAs in the test mixture bins.\n")
        browser()
      }

      # For the outcome regression exclude the original exposures in this group
      # plus the outcome variable, and add in the mixture bins.
      # TODO: determine if we do want to include the other exposure groups.
      vars = setdiff(names(data_train), c(outcome, exposure_names))
      #vars = setdiff(names(data_train), exposure_names)
      df_outcome_reg = cbind(data_train[, vars, drop = FALSE],
                             # Mixture_bins is a factor but we just need the integer codes.
                             # TODO: perhaps this should be one-hot encoded instead?
                             mixture_bins = as.integer(mixture_bins))

      # Estimate pooled outcome regression that includes all of the quantiles.
      # E[ Y | mixture bins, adjustment variables]
      # TODO: support arbitrary estimators, like sl3, mlr, caret, etc.
      reg_outcome =
        SuperLearner(Y = data_train[[outcome]],
                     X = df_outcome_reg,
                     family = family,
                     cvControl = SuperLearner.CV.control(V = folds_sl,
                                                         # Stratify if we have a binary outcome.
                                                         stratifyCV = family == "binomial"),
                     SL.library = estimator_outcome)

      if (verbose) {
        cat("Outcome regression result:\n")
        print(reg_outcome)
      }

      # We can save the propensity estimators for each quantile.
      # Note though, that we are directly applying to test data so saving
      # seems pretty optional. Yet may be useful for later analysis.
      regs_propensity = list()

      test_result_df = NULL

      # Loop over bins.
      for (bin_i in seq(quantiles_mixtures)) {
        # (Could estimate outcome regression per mixture bin, but using pooled currently.)

        # This is our A
        is_current_bin = as.integer(as.integer(mixture_bins) == bin_i)


        # Check if all observations have the same value for is_current_bin, in which
        # case we can't do propensity score estimation.
        # TODO: why would this happen - is something else going wrong?
        if (length(unique(is_current_bin)) == 1L) {
          warning(paste0("Warning: is_current_bin distribution has a single value.",
                         "\nSkipping propensity score estimation for bin ", bin_i))
          browser()
          reg_propensity = NULL
        } else {


          # Estimate propensity score regression per mixture_bin.
          # This will give an error "stratifyCV only implemented for binary Y" if all
          # observations are 1 or all are 0.
          reg_propensity =
            # Suppress warnings like "In lognet(x, is.sparse, ix, jx, y, weights, offset, alpha,  ... :
            # one multinomial or binomial class has fewer than 8  observations; dangerous ground"
            # TODO: only suppress specific warnings, like Oleg has in stremr.
            suppressWarnings(
            # We convert mixture_bins to an integer because it's a factor.
            SuperLearner(Y = is_current_bin,
                         X = subset(df_outcome_reg, select = -c(mixture_bins)),
                         family = "binomial",
                         cvControl = SuperLearner.CV.control(V = folds_sl,
                                                             stratifyCV = TRUE),
                         SL.library = estimator_propensity)
            )

          if (verbose) {
            cat("Propensity regression result", paste0("(", bin_i, "):\n"))
            print(reg_propensity)
          }

        }

        # Save fits to apply to test data.
        regs_propensity[[bin_i]] = reg_propensity


        ###############
        # Also apply directly to test data here.

        # Dataframe to hold our counterfactuals.
        df_outcome = data_test[, setdiff(names(data_test), c(outcome, exposure_names))]

        # Set all obs to have this mixture bin level.
        df_outcome$mixture_bins = bin_i

        # Predict Q(1, W) - all observations have this mixture level.
        if (class(reg_outcome) == "SuperLearner") {
          q_pred = predict(reg_outcome, df_outcome, onlySL = TRUE)$pred
        } else {
          q_pred = predict(reg_outcome, df_outcome)
        }

        df_propensity = subset(df_outcome, select = -c(mixture_bins))

        # Predict g - propensity to have this mixture level.
        if (is.null(reg_propensity)) {
          # We have a null propensity score estimator, meaning the test data had no
          # variation in the membership in this bin
          g_pred = NULL
        } else if (class(reg_propensity) == "SuperLearner") {
          # TODO: is type = "response" needed for SuperLearner? Need to check.
          g_pred = predict(reg_propensity, df_propensity, onlySL = TRUE,
                           type = "response")$pred
        } else {
          g_pred = predict(reg_propensity, df_propensity, type = "response")
        }

        # TODO: make g_min a function argument.
        g_min = 0.00001

        # Trunctate g_hat
        g_pred = bound(g_pred, c(g_min, 1 - g_min))


        # Create clever covariate
        h1w = 1 / g_pred

        # is_current_bin = A (treatment indicator).
        test_treatment_indicator = as.integer(as.integer(mixture_bins_test) == bin_i)

        # Set NAs to 0 for now.
        # TODO: figure out why some are NAs - just NA in the data? Or due to quantiles.
        # TODO: use actual adjustment for missingness, or another solution.
        test_treatment_indicator[is.na(test_treatment_indicator)] = 0L

        #browser()

        haw = test_treatment_indicator * h1w

        # Confirm we have no NAs in our haw vector.
        stopifnot(sum(is.na(haw)) == 0L)

        # NOTE: we don't calculate IC here because we need to do CV-TMLE fluctuation
        # using all folds' results (later).

        # Create dataframe of results for this mixture quantile.
        new_result = data.frame(quantile = bin_i,
                                # TODO: calculate and return Y_star
                                y = data_test[[outcome]],
                                # This is our A for this quantile.
                                in_quantile = test_treatment_indicator,
                                q_pred, g_pred, h1w, haw)

        # Append to our tracking dataframe.
        test_result_df = rbind(test_result_df, new_result)

      }

      if (FALSE && length(mixture_test) != nrow(test_result_df)) {
        cat("Error: test mixture length", length(mixture_test),
        "does not match test_result_df rows", nrow(test_result_df), ".\n")
        browser()
      }

      # Save mixture prediction on stacked test data.
      test_result_df[[paste0("mixture_", group_name)]] = mixture_test

      # Save the stacked set of mixture quantile results for this exposure group.
      test_results[[group_name]] = test_result_df

      group_status[[group_name]] = list(
        status = "succeeded",
        message = NULL
      )
    }

    # Compile results.
    results =
      list(weights = weights,
           # Contains the exposure groups that we created on the training data.
           # (Not yet implemented)
           exposure_groups = exposure_groups,
           group_status = group_status,
           test_results = test_results,
           cutpoints = cutpoints)

    return(results)

}
